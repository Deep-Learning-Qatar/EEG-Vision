{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG-Net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i24y39obsBa_",
        "xo7_7JvuFkBn",
        "Ez9FVTJV2f9i",
        "scSI6ZTysSkm",
        "0vcqWzaP6lJ4",
        "VRh8l4BK_FLl",
        "v5a08ikp14tA",
        "6xQU7Fel74eJ",
        "wcp9GjvBGgC1",
        "BfkCrCucZpSb",
        "3D9ogJBnnsdr",
        "XTwnizbqDeSV",
        "6dJuDniDMwd5"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPN7ruxnvBz5Wkval/uJyE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deep-Learning-Qatar/EEG-Vision/blob/main/EEG_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nybk-Cstr0Ox"
      },
      "source": [
        "# **Connect to drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxQDmq1Yr40q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9012c1-1d44-4a37-c538-24d13d5bb962"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i24y39obsBa_"
      },
      "source": [
        "# **Import required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUvBbPotrBR4"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7_7JvuFkBn"
      },
      "source": [
        "# **Are you using GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et7A1_23FrJ6"
      },
      "source": [
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez9FVTJV2f9i"
      },
      "source": [
        "# **Move the data to local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5XmIjMq4ItT"
      },
      "source": [
        "# change based on where you store your data and where you are gonna load it from\n",
        "%mkdir data\n",
        "!cp drive/MyDrive/dl_project_data/eeg_55_95_std.pth data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAiAsZ_02pFZ"
      },
      "source": [
        "data_path = 'data/eeg_55_95_std.pth'\n",
        "data_dict = torch.load(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scSI6ZTysSkm"
      },
      "source": [
        "# **Hyper-parameters of Bi-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD6FcmMYsms8"
      },
      "source": [
        "bidirectional    = True \n",
        "lr               = 0.01\n",
        "wd               = 5e-6\n",
        "dropout_prob     = 0.5  \n",
        "record_length    = 440    # Fares et al. (2019)\n",
        "batch_size       = 440    # Fares et al. (2019)\n",
        "input_size       = 128    # Fares et al. (2019)\n",
        "feat_num         = 60     # Fares et al. (2019)  \n",
        "num_classes      = 40     # Fares et al. (2019)\n",
        "num_hidden_nodes = 128    # Fares et al. (2019) / might be different for us (128)\n",
        "num_layers       = 2      # Fares et al. (2019)\n",
        "num_epochs       = 2500   # Fares et al. (2019)\n",
        "ICA_iteratoins   = 400    # Fares et al. (2019)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcqWzaP6lJ4"
      },
      "source": [
        "# **Independent component analysis classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qciv-5ZQ6rNU"
      },
      "source": [
        "# source: https://towardsdatascience.com/independent-component-analysis-ica-in-python-a0ef0db0955e\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def g(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def g_der(x):\n",
        "    return 1 - g(x) * g(x)\n",
        "\n",
        "def center(X):\n",
        "    X = np.array(X)\n",
        "    mean = X.mean(axis=1, keepdims=True)\n",
        "    return X- mean\n",
        "\n",
        "def whitening(X):\n",
        "    cov = np.cov(X)\n",
        "    d, E = np.linalg.eigh(cov)\n",
        "    D = np.diag(d)\n",
        "    D_inv = np.sqrt(np.linalg.inv(D))\n",
        "    X_whiten = np.dot(E, np.dot(D_inv, np.dot(E.T, X)))\n",
        "    return X_whiten\n",
        "\n",
        "def calculate_new_w(w, X):\n",
        "    w_new = (X * g(np.dot(w.T, X))).mean(axis=1) - g_der(np.dot(w.T, X)).mean() * w\n",
        "    w_new /= np.sqrt((w_new ** 2).sum())\n",
        "    return w_new\n",
        "\n",
        "def ica(X, iterations, tolerance=1e-5):\n",
        "    X = center(X)\n",
        "    X = whitening(X)\n",
        "    components_nr = X.shape[0]\n",
        "    \n",
        "    W = np.zeros((components_nr, components_nr), dtype=X.dtype)\n",
        "    for i in range(components_nr):        \n",
        "            w = np.random.rand(components_nr)\n",
        "            \n",
        "            for j in range(iterations):\n",
        "                w_new = calculate_new_w(w, X)\n",
        "                if i >= 1:\n",
        "                    w_new -= np.dot(np.dot(w_new, W[:i].T), W[:i])\n",
        "               \n",
        "                distance = np.abs(np.abs((w * w_new).sum()) - 1)\n",
        "                w = w_new\n",
        "                if distance < tolerance:\n",
        "                    break\n",
        "\n",
        "            W[i, :] = w\n",
        "    S = np.dot(W, X)\n",
        "    return S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRh8l4BK_FLl"
      },
      "source": [
        "# **Architecture of Bi-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4uVibpM_SD8"
      },
      "source": [
        "# BiLSTM for EEG features encoding 95% accuracy\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, record_length, input_size, nodes_num, feat_num, num_classes, n_layers, bidirectional, dropout):\n",
        "        #                  440,            128      128          60      40,         2           T           0.2\n",
        "        super().__init__()\n",
        "        \n",
        "        # feature encoding layers\n",
        "\n",
        "        # sequence input layer\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            nn.Conv1d(input_size, nodes_num, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # Stacked BiLSTM netwrok\n",
        "        self.lstm = nn.LSTM(nodes_num,\n",
        "                            nodes_num,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout,\n",
        "                            bias=True)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(nodes_num * 2, nodes_num) #x2 because we num_directions=2\n",
        "        \n",
        "        # Relu layer\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # classification layer\n",
        "        self.fc2 = nn.Linear(nodes_num, num_classes)\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "\n",
        "        # preprocessing to pass it to CNN (B x * x T)\n",
        "        x2 = x.permute(0, 2, 1)\n",
        "\n",
        "        # through CNN\n",
        "        embedded = self.embedding(x2)            # (B x * x T)\n",
        "        embedded = embedded.permute(0, 2, 1)    # (B x * x T)\n",
        "\n",
        "        # through BiLSTM\n",
        "        packed_embedded = pack_padded_sequence(embedded, x_len, batch_first=True) \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "\n",
        "        # Combine output of BiLSTM and extract features\n",
        "        cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        features = self.fc1(cat)\n",
        "        features_out =  self.dropout(self.relu(features))\n",
        "\n",
        "        #classify \n",
        "        class_out = self.dropout(self.fc2(features))\n",
        "        \n",
        "        return features_out, class_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5a08ikp14tA"
      },
      "source": [
        "# **Data pre-processsing and Data-set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-2pofgGWaxr"
      },
      "source": [
        "# Load data and create dataset for each split\n",
        "def split_train_val_test(data_dict, splits=(0.8, 0.1, 0.1)):\n",
        "\n",
        "    # Find all possible image IDs\n",
        "    image_ids = set()\n",
        "    for di in data_dict['dataset']:\n",
        "        image_ids.add(di['image'])\n",
        "    # print(len(image_ids))\n",
        "\n",
        "    # Organise data by image (key: image ID, val: list of data dicts with all data)\n",
        "    data_by_image = dict()\n",
        "    for id in image_ids:\n",
        "        data_by_image[id] = []\n",
        "    for di in data_dict['dataset']:\n",
        "        image_id = di['image']\n",
        "        data_by_image[image_id].append(di)\n",
        "\n",
        "    # Shuffle data so selection for splits are random\n",
        "    image_ids_li = list(image_ids)\n",
        "    random.shuffle(image_ids_li)\n",
        "    data_by_image = {id: data_by_image[id] for id in image_ids_li}\n",
        "    \n",
        "    # Create val and test sets\n",
        "    data_len = len(image_ids_li)\n",
        "    val_len, test_len = int(splits[1]*data_len), int(splits[2]*data_len)\n",
        "    val_data, test_data = dict(), dict()\n",
        "    for i in range(val_len):\n",
        "        k, v = data_by_image.popitem()\n",
        "        val_data[k] = v\n",
        "    for i in range(test_len):\n",
        "        k, v = data_by_image.popitem()\n",
        "        test_data[k] = v\n",
        "    train_data = data_by_image\n",
        "    # print(train_data.keys())\n",
        "    # print(val_data.keys())\n",
        "    # print(test_data.keys())\n",
        "    \n",
        "    # Return all sets\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "# General data set for EEG data dictionary\n",
        "class EEGDataSet(data.Dataset):\n",
        "    \"\"\"\n",
        "    Possible labels: eeg, label (ID), image (ID), subject\n",
        "    Interval indicates what section of the ~500ms EEG signal should be returned\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dict, x_label='eeg', y_label='label', interval=(20, 460)):\n",
        "        dl = list(data_dict.values())\n",
        "        self.data_list = [item for sublist in dl for item in sublist]\n",
        "        self.length = len(self.data_list)\n",
        "        self.interval = interval\n",
        "        self.x_label = x_label\n",
        "        self.y_label = y_label\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data_entry = self.data_list[index]\n",
        "        x = data_entry[self.x_label]\n",
        "        if self.x_label == 'eeg':\n",
        "            x = x[:, self.interval[0]:self.interval[1]]\n",
        "            x = torch.transpose(x, 0, 1).float()\n",
        "        else:\n",
        "            x = torch.as_tensor(x).float()\n",
        "        y = data_entry[self.y_label]\n",
        "        if self.y_label == 'eeg':\n",
        "            y = y[:, self.interval[0]:self.interval[1]]\n",
        "            y = torch.transpose(y, 0, 1).long()\n",
        "        else:\n",
        "            y = torch.as_tensor(y).long()\n",
        "        return x, y\n",
        "\n",
        "    # def collate(self, batch):\n",
        "    #     # Seperate data and labels\n",
        "    #     X = [x[0] for x in batch]\n",
        "    #     Y = [y[1] for y in batch]\n",
        "\n",
        "    #     # Get lengths\n",
        "    #     X_length = [batch_size for i in X]\n",
        "    #     # X_length = torch.LongTensor([len(i) for i in X])\n",
        "\n",
        "    #     return torch.tensor(X), torch.tensor(Y), X_length\n",
        "\n",
        "\n",
        "train_data, val_data, test_data = split_train_val_test(data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xQU7Fel74eJ"
      },
      "source": [
        "# **Load data into Data-loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35PKUSp_781V"
      },
      "source": [
        "# Training\n",
        "train_dataset = EEGDataSet(train_data)\n",
        "train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=4) if cuda else dict(shuffle=True, batch_size=batch_size)\n",
        "train_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
        "\n",
        "# Validation\n",
        "val_dataset = EEGDataSet(val_data)\n",
        "val_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=4) if cuda else dict(shuffle=False, batch_size=batch_size)\n",
        "val_loader = data.DataLoader(val_dataset, **val_loader_args)\n",
        "\n",
        "# Testing\n",
        "test_dataset = EEGDataSet(test_data)\n",
        "test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=4) if cuda else dict(shuffle=False, batch_size=batch_size)\n",
        "test_loader = data.DataLoader(test_dataset, **test_loader_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcp9GjvBGgC1"
      },
      "source": [
        "# **Set up the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMsuvsFGZ9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0298a9e7-2ace-4fa9-c8b0-4593e01e1d40"
      },
      "source": [
        "model = BiLSTM(record_length, input_size, num_hidden_nodes, feat_num, num_classes, num_layers, bidirectional, dropout_prob)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")                        \n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BiLSTM(\n",
            "  (embedding): Sequential(\n",
            "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=40, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfkCrCucZpSb"
      },
      "source": [
        "# **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtlFwg6lnSl_"
      },
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    print(\"Training...\")\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader): \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        data = (data).to(device)\n",
        "        target = (target).to(device)\n",
        "        X_length = [batch_size for i in data]\n",
        "\n",
        "        features, outputs = model(data, X_length) # Run data through the model\n",
        "        loss = criterion(outputs, target)            # Compare output to target\n",
        "        running_loss += loss.item()                  # Add loss to the total\n",
        "\n",
        "        loss.backward()                              # Backpropagate the loss\n",
        "        optimizer.step()                             # Update the weights & biases\n",
        "\n",
        "        for i in range(len(outputs)):\n",
        "            if torch.argmax(outputs[i]) == target[i]:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    running_loss /= len(train_loader)\n",
        "    print('Training Loss: ', running_loss, ' Time: ', end_time-start_time, 's')\n",
        "    acc = (correct_predictions/total_predictions)*100.0\n",
        "    print('Training Accuracy: ', acc, '%')\n",
        "    return running_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D9ogJBnnsdr"
      },
      "source": [
        "# **Validation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjEvBOMcnsHi"
      },
      "source": [
        "def val_model(model, val_loader, criterion):\n",
        "    print(\"Validating...\")\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        total_predictions = 0.0\n",
        "        correct_predictions = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):\n",
        "\n",
        "            data = (data).to(device)\n",
        "            target = (target).to(device)\n",
        "            X_length = [batch_size for i in data]\n",
        "\n",
        "            features, outputs = model(data, X_length) # Run data through the model\n",
        "            loss = criterion(outputs, target).detach()    # Compare output to target\n",
        "            running_loss += loss.item()                   # Add loss to the total\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "                if torch.argmax(outputs[i]) == target[i]:\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "\n",
        "\n",
        "        running_loss /= len(val_loader)\n",
        "        acc = (correct_predictions/total_predictions)*100.0\n",
        "        print('Validating Loss: ', running_loss)\n",
        "        print('Validating Accuracy: ', acc, '%')\n",
        "        return running_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTwnizbqDeSV"
      },
      "source": [
        "# **Train & Validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Zy-DE3ocGb"
      },
      "source": [
        "Train_loss  = []\n",
        "Train_acc   = []\n",
        "Val_loss    = []\n",
        "Val_acc     = []\n",
        "trialNumber = 3\n",
        "\n",
        "# train and get results\n",
        "for i in range(num_epochs):\n",
        "    print(\"Epoch Number: \", i)\n",
        "    t_loss, t_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    v_loss, v_acc = val_model(model, val_loader, criterion)\n",
        "    scheduler.step(v_loss)\n",
        "\n",
        "    # Add results\n",
        "    Train_loss.append(t_loss)\n",
        "    Val_loss.append(v_loss)\n",
        "    Train_acc.append(t_acc)\n",
        "    Val_acc.append(v_acc)\n",
        "\n",
        "    # Save model (epoch)\n",
        "    torch.save(model.state_dict(), 'drive/MyDrive/dl_project_data/results/run_'+str(trialNumber)+'/model_'+\"epoch\"+str(i)+'.pt')\n",
        "    print(\"\")\n",
        "    print(\"===\"*20)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dJuDniDMwd5"
      },
      "source": [
        "# **Plotting the resuls**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8es7PixbQdxM"
      },
      "source": [
        "\n",
        "# PLot Training Loss\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir9gBW8HQycx"
      },
      "source": [
        "\n",
        "# PLot Validating Loss\n",
        "plt.title('Validating Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Val_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgmhpFsrQ0-6"
      },
      "source": [
        "\n",
        "#PLot Training Accuracy\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(Train_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjTGBMyuQ20c"
      },
      "source": [
        "#PLot Validating Accuracy\n",
        "plt.title('Validatin Accuracy')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(Val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}