# -*- coding: utf-8 -*-
"""EEG-Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Deep-Learning-Qatar/EEG-Vision/blob/main/EEG_Net.ipynb

"""

"""# **Import required Libraries**"""

import time
import torch
import random
import numpy as np
import torch.nn as nn
from torch.utils import data
import matplotlib.pyplot as plt
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import copy


"""# **Hyper-parameters of Bi-LSTM**"""

bidirectional    = True 
# lr               = 0.01
# wd               = 5e-6
# dropout_prob     = 0.2
record_length    = 440    # Fares et al. (2019)
batch_size       = 440    # Fares et al. (2019)
input_size       = 128    # Fares et al. (2019)
feat_num         = 60     # Fares et al. (2019)  
# num_classes      = 40     # Fares et al. (2019)
num_hidden_nodes = 128    # Fares et al. (2019) / might be different for us (128)
num_layers       = 2      # Fares et al. (2019)
# num_epochs       = 2500   # Fares et al. (2019)
ICA_iteratoins   = 400    # Fares et al. (2019)

"""# **Independent component analysis classifier**"""

# source: https://towardsdatascience.com/independent-component-analysis-ica-in-python-a0ef0db0955e

np.random.seed(0)

def g(x):
    return np.tanh(x)

def g_der(x):
    return 1 - g(x) * g(x)

def center(X):
    X = np.array(X)
    mean = X.mean(axis=1, keepdims=True)
    return X- mean

def whitening(X):
    cov = np.cov(X)
    d, E = np.linalg.eigh(cov)
    D = np.diag(d)
    D_inv = np.sqrt(np.linalg.inv(D))
    X_whiten = np.dot(E, np.dot(D_inv, np.dot(E.T, X)))
    return X_whiten

def calculate_new_w(w, X):
    w_new = (X * g(np.dot(w.T, X))).mean(axis=1) - g_der(np.dot(w.T, X)).mean() * w
    w_new /= np.sqrt((w_new ** 2).sum())
    return w_new

def ica(X, iterations, tolerance=1e-5):
    X = center(X)
    X = whitening(X)
    components_nr = X.shape[0]
    
    W = np.zeros((components_nr, components_nr), dtype=X.dtype)
    for i in range(components_nr):        
            w = np.random.rand(components_nr)
            
            for j in range(iterations):
                w_new = calculate_new_w(w, X)
                if i >= 1:
                    w_new -= np.dot(np.dot(w_new, W[:i].T), W[:i])
               
                distance = np.abs(np.abs((w * w_new).sum()) - 1)
                w = w_new
                if distance < tolerance:
                    break

            W[i, :] = w
    S = np.dot(W, X)
    return S

"""# **Architecture of Bi-LSTM**"""

# BiLSTM for EEG features encoding 95% accuracy
class BiLSTM(nn.Module):
    def __init__(self, record_length, input_size, nodes_num, feat_num, num_classes, n_layers, bidirectional, dropout):
        #                  440,            128      128          60      40,         2           T           0.2
        super().__init__()
        
        # feature encoding layers

        # sequence input layer
        self.embedding = torch.nn.Sequential(
            nn.Conv1d(input_size, nodes_num, 3, padding=1, bias=False),
            nn.BatchNorm1d(input_size),
            nn.ReLU(inplace=True))

        # Stacked BiLSTM netwrok
        self.lstm = nn.LSTM(nodes_num,
                            nodes_num,
                            num_layers=n_layers,
                            bidirectional=bidirectional,
                            batch_first=True,
                            dropout=dropout,
                            bias=True)
        
        # Fully connected layer
        self.fc1 = nn.Linear(nodes_num * 2, nodes_num) #x2 because we num_directions=2
        
        # Relu layer
        self.relu = nn.ReLU()
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # classification layer
        self.fc2 = nn.Linear(nodes_num, num_classes)

    def forward(self, x, x_len):

        # preprocessing to pass it to CNN (B x * x T)
        x2 = x.permute(0, 2, 1)

        # through CNN
        embedded = self.embedding(x2)            # (B x * x T)
        embedded = embedded.permute(0, 2, 1)    # (B x * x T)

        # through BiLSTM
        packed_embedded = pack_padded_sequence(embedded, x_len, batch_first=True) 
        packed_output, (hidden, cell) = self.lstm(packed_embedded)

        # Combine output of BiLSTM and extract features
        cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)
        features = self.fc1(cat)
        features_out =  self.dropout(self.relu(features))

        #classify 
        class_out = self.dropout(self.fc2(features))
        
        return features_out, class_out

"""# **Data pre-processsing and Data-set**"""

# General data set for EEG data dictionary
class EEGDataSet(data.Dataset):
    """
    Possible labels: eeg, label (ID), image (ID), subject
    Interval indicates what section of the ~500ms EEG signal should be returned
    """
    def __init__(self, data_dict, x_label='eeg', y_label='label', interval=(20, 460)):
        dl = list(data_dict.values())
        self.data_list = [item for sublist in dl for item in sublist]
        self.length = len(self.data_list)
        self.interval = interval
        self.x_label = x_label
        self.y_label = y_label
        
    def __len__(self):
        return self.length
    
    def __getitem__(self, index):
        data_entry = self.data_list[index]
        x = data_entry[self.x_label]
        if self.x_label == 'eeg':
            x = x[:, self.interval[0]:self.interval[1]]
            x = torch.transpose(x, 0, 1).float()
        else:
            x = torch.as_tensor(x).float()
        y = data_entry[self.y_label]
        if self.y_label == 'eeg':
            y = y[:, self.interval[0]:self.interval[1]]
            y = torch.transpose(y, 0, 1).long()
        else:
            y = torch.as_tensor(y).long()
        return x, y

"""# Module functions """
def make_EEG_data_loaders(config, eeg_path):
    eeg_datasets = torch.load(eeg_path)

    # Training
    train_dataset = EEGDataSet(eeg_datasets['train'])
    train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=2)
    train_loader = data.DataLoader(train_dataset, **train_loader_args)

    # Validation
    val_dataset = EEGDataSet(eeg_datasets['val'])
    val_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=2)
    val_loader = data.DataLoader(val_dataset, **val_loader_args)

    # Testing
    test_dataset = EEGDataSet(eeg_datasets['test'])
    test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=2)
    test_loader = data.DataLoader(test_dataset, **test_loader_args)

    dataloaders_dict = dict(
        train=train_loader,
        val=val_loader,
        test=test_loader,
    )

    del eeg_datasets

    return dataloaders_dict

def make_EEG_Net(config, eeg_path):

    dataloaders = make_EEG_data_loaders(config, eeg_path)

    """# **Set up the model**"""
    if config.model == 'fares':
        model = BiLSTM(record_length, input_size, num_hidden_nodes, feat_num, config.num_classes, num_layers, bidirectional, config.dropout)
    else:
        model = None

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)                     

    model.cuda()

    return model, dataloaders, criterion, optimizer, scheduler

def train_and_val_EEG_Net(wandb, config, model, dataloaders, criterion, optimizer, scheduler, num_epochs=2500):

    since = time.time()

    val_acc_history = []

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    epoch_loss = None

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.cuda()
                labels = labels.cuda()
                x_len = [batch_size]*len(inputs)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss
                    features, outputs = model(inputs, x_len)
                    loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            if phase == 'train':
                wandb.log({"epoch": epoch, "train_loss": epoch_loss, "train_acc": epoch_acc})
            elif phase == 'val':
                wandb.log({"epoch": epoch, "val_loss": epoch_loss, "val_acc": epoch_acc})
            else:
                print('Did not log')
            
            if epoch%(int(num_epochs/10)) == 1:
                print('[{}] Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
            if phase == 'val':
                val_acc_history.append(epoch_acc)

        
        print()

        # Save model every epoch
        filename = 'EEGNet' + str(config['model_nr']) + 'epoch' + str(epoch+1) + '.pth'
        torch.save(model.state_dict(), filename)

        scheduler.step(epoch_loss)

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model, val_acc_history

def test_EEG_Net(model, test_loader):
    model.eval()

    # Run the model on some test examples
    with torch.no_grad():
        correct, total = 0., 0
        predictions = []

        for feats, labels in test_loader:
            feats, labels = feats.cuda(), labels.cuda()
            
            outputs = model(feats)

            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            predictions.append(predicted.cpu().numpy())
            
            del feats
            del labels

        acc = correct / total
        return predictions, acc


